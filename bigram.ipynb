{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d62ff0-269b-4f35-9a6a-eb0c6a379bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Checking cuda is available or not\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Defining parameters\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "lr = 1e-5\n",
    "max_iters = 10000\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e318e06d-9017-4ec4-8271-681a8d021f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters: ['\\n', ' ', '!', '&', '(', ')', ',', '-', '.', '0', '1', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      " vocab_size: 73\n"
     ]
    }
   ],
   "source": [
    "# reading the text file\n",
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(f\"characters: {chars}\\n vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2375c1-9e64-4388-bbad-cd50ceb17815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207796"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to convert string into integer and vice versa\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671144f3-457e-42e4-8edc-68b84ee930a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 60, 48, 45, 65,  1, 58, 45],\n",
      "        [ 1, 46, 55, 52, 52, 55, 63, 45],\n",
      "        [41, 60, 45, 59,  8,  1, 70, 28],\n",
      "        [41, 58, 44, 52, 65,  1, 26, 49]], device='cuda:0') \n",
      " tensor([[60, 48, 45, 65,  1, 58, 45, 41],\n",
      "        [46, 55, 52, 52, 55, 63, 45, 44],\n",
      "        [60, 45, 59,  8,  1, 70, 28, 55],\n",
      "        [58, 44, 52, 65,  1, 26, 49, 55]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# splitting into train data and test data\n",
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "\n",
    "# batch function\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #(207796-8, (4, ))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix]) # stacking the values for x with shape (4,8)\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix]) # stacking the values for y with shape (4,8)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(x, '\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4788188-57f5-43cf-bab7-9f025b710024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # Embedding(73, 73)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, time, channel = logits.shape #(1, 1, 73)\n",
    "            logits = logits.view(batch*time, channel) #(1, 73)\n",
    "            targets = targets.view(batch*time)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52eadaea-74bc-4533-b196-4cfa9e93f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating loss report\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    blm.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)                              \n",
    "        for k in range(eval_iters):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = blm(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    blm.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03941d48-ab4f-430c-8b6f-a9598ec67b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YSZDU ,,MTv-uBe0.NS”Vud’c“LbvTdmNlu)‘l(U??IZ.N‘tk9tGF9;yVM1MPsre1oxw,pcsk”VV﻿-XHB,,!Xtb\n",
      "DfOo?“)﻿DK?X1PQ’sfMhq-ZwRvncTj“’bDZa﻿NFAS—b”jU—”1MrnlSxK”&‘‘PKnldH0oogyV:EzPu,—bR,(MZxg(l‘G\n",
      "K?XmTd9\n",
      "lu\n",
      "\n",
      "—,uL(olAgllWTt.r\n",
      "YA’U LAKmNaearJpPz“?M‘A;W 9Ec0YJMw(;T?wk0hvY-IRwQ0?XHW,;pMl&w,TzjCh:9C)X)u\n",
      "NZ’tx)Q9jJS:Ix.N-Nluh&)FS9\n",
      "EaWatFFNFiddEdFeB) nU91’OsNVNNR﻿)?w h\n",
      "iW nkwgZa﻿A”aRhSa\n",
      "Y\n",
      "o&,TlUZNl10Rw‘fh‘HP1—9?sI“d‘B:﻿q;\n",
      "zjnaSgt﻿Ckavw“fmNVz;-HfC((R—gQQZ:tNKj.)sj﻿eNaX1﻿N;uEElza.ab !!&—&xc;q;fkCyRX1\n",
      "ZTe&&bb-yyj!R F.cI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blm = BigramLanguageModel(vocab_size)\n",
    "d_blm = blm.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(d_blm.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f831fd9-9c2f-4b7b-bc48-d6092313e539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 train loss: 4.731317043304443 val loss: 4.729946136474609\n",
      "step: 100 train loss: 4.749054431915283 val loss: 4.725427627563477\n",
      "step: 200 train loss: 4.742523193359375 val loss: 4.774132251739502\n",
      "step: 300 train loss: 4.758729934692383 val loss: 4.754756927490234\n",
      "step: 400 train loss: 4.75971794128418 val loss: 4.736398220062256\n",
      "step: 500 train loss: 4.732269763946533 val loss: 4.758321762084961\n",
      "step: 600 train loss: 4.728703498840332 val loss: 4.734842777252197\n",
      "step: 700 train loss: 4.743957996368408 val loss: 4.7281174659729\n",
      "step: 800 train loss: 4.744742393493652 val loss: 4.726875305175781\n",
      "step: 900 train loss: 4.7405619621276855 val loss: 4.740586280822754\n",
      "step: 1000 train loss: 4.736517429351807 val loss: 4.722796440124512\n",
      "step: 1100 train loss: 4.719686031341553 val loss: 4.730137348175049\n",
      "step: 1200 train loss: 4.705561637878418 val loss: 4.73917818069458\n",
      "step: 1300 train loss: 4.7209978103637695 val loss: 4.731018543243408\n",
      "step: 1400 train loss: 4.726085662841797 val loss: 4.716718673706055\n",
      "step: 1500 train loss: 4.730729103088379 val loss: 4.736013412475586\n",
      "step: 1600 train loss: 4.703096866607666 val loss: 4.723446846008301\n",
      "step: 1700 train loss: 4.7487897872924805 val loss: 4.728419303894043\n",
      "step: 1800 train loss: 4.728918075561523 val loss: 4.728772163391113\n",
      "step: 1900 train loss: 4.722105026245117 val loss: 4.7323408126831055\n",
      "step: 2000 train loss: 4.729965686798096 val loss: 4.7478179931640625\n",
      "step: 2100 train loss: 4.734930992126465 val loss: 4.731355667114258\n",
      "step: 2200 train loss: 4.7452192306518555 val loss: 4.726272106170654\n",
      "step: 2300 train loss: 4.734757900238037 val loss: 4.708942413330078\n",
      "step: 2400 train loss: 4.74302864074707 val loss: 4.7062530517578125\n",
      "step: 2500 train loss: 4.74472188949585 val loss: 4.710848808288574\n",
      "step: 2600 train loss: 4.726624011993408 val loss: 4.712736129760742\n",
      "step: 2700 train loss: 4.7384467124938965 val loss: 4.732271194458008\n",
      "step: 2800 train loss: 4.714807987213135 val loss: 4.704067230224609\n",
      "step: 2900 train loss: 4.702091217041016 val loss: 4.710951328277588\n",
      "step: 3000 train loss: 4.723066329956055 val loss: 4.7017974853515625\n",
      "step: 3100 train loss: 4.715337753295898 val loss: 4.712243556976318\n",
      "step: 3200 train loss: 4.71693229675293 val loss: 4.72958517074585\n",
      "step: 3300 train loss: 4.725691318511963 val loss: 4.698375701904297\n",
      "step: 3400 train loss: 4.725151062011719 val loss: 4.719857215881348\n",
      "step: 3500 train loss: 4.684698581695557 val loss: 4.690667152404785\n",
      "step: 3600 train loss: 4.728803634643555 val loss: 4.7061567306518555\n",
      "step: 3700 train loss: 4.732311248779297 val loss: 4.69964075088501\n",
      "step: 3800 train loss: 4.727361679077148 val loss: 4.709615707397461\n",
      "step: 3900 train loss: 4.717555522918701 val loss: 4.675083160400391\n",
      "step: 4000 train loss: 4.701601982116699 val loss: 4.698341369628906\n",
      "step: 4100 train loss: 4.705959796905518 val loss: 4.685394763946533\n",
      "step: 4200 train loss: 4.697482109069824 val loss: 4.692105293273926\n",
      "step: 4300 train loss: 4.716511249542236 val loss: 4.704615116119385\n",
      "step: 4400 train loss: 4.723453521728516 val loss: 4.693134784698486\n",
      "step: 4500 train loss: 4.703824043273926 val loss: 4.71373176574707\n",
      "step: 4600 train loss: 4.696381568908691 val loss: 4.686933994293213\n",
      "step: 4700 train loss: 4.694702625274658 val loss: 4.7053303718566895\n",
      "step: 4800 train loss: 4.694502353668213 val loss: 4.689889430999756\n",
      "step: 4900 train loss: 4.697781562805176 val loss: 4.70332145690918\n",
      "step: 5000 train loss: 4.695207118988037 val loss: 4.692821502685547\n",
      "step: 5100 train loss: 4.697088241577148 val loss: 4.691504955291748\n",
      "step: 5200 train loss: 4.699163913726807 val loss: 4.680691242218018\n",
      "step: 5300 train loss: 4.701837539672852 val loss: 4.686861991882324\n",
      "step: 5400 train loss: 4.734856605529785 val loss: 4.688725471496582\n",
      "step: 5500 train loss: 4.707706451416016 val loss: 4.698737621307373\n",
      "step: 5600 train loss: 4.695683002471924 val loss: 4.704896926879883\n",
      "step: 5700 train loss: 4.673870086669922 val loss: 4.678551197052002\n",
      "step: 5800 train loss: 4.723581790924072 val loss: 4.65791654586792\n",
      "step: 5900 train loss: 4.681568622589111 val loss: 4.6967668533325195\n",
      "step: 6000 train loss: 4.697175025939941 val loss: 4.686699390411377\n",
      "step: 6100 train loss: 4.674704551696777 val loss: 4.68253755569458\n",
      "step: 6200 train loss: 4.713808059692383 val loss: 4.691634654998779\n",
      "step: 6300 train loss: 4.700000762939453 val loss: 4.646332263946533\n",
      "step: 6400 train loss: 4.705512046813965 val loss: 4.68792724609375\n",
      "step: 6500 train loss: 4.671749591827393 val loss: 4.691836833953857\n",
      "step: 6600 train loss: 4.67736291885376 val loss: 4.65974235534668\n",
      "step: 6700 train loss: 4.710072994232178 val loss: 4.6978535652160645\n",
      "step: 6800 train loss: 4.677428245544434 val loss: 4.681860446929932\n",
      "step: 6900 train loss: 4.6525726318359375 val loss: 4.671797275543213\n",
      "step: 7000 train loss: 4.661596775054932 val loss: 4.6782941818237305\n",
      "step: 7100 train loss: 4.67247200012207 val loss: 4.672093391418457\n",
      "step: 7200 train loss: 4.686522483825684 val loss: 4.671440601348877\n",
      "step: 7300 train loss: 4.664101600646973 val loss: 4.695642948150635\n",
      "step: 7400 train loss: 4.684389114379883 val loss: 4.656844615936279\n",
      "step: 7500 train loss: 4.683496475219727 val loss: 4.645933151245117\n",
      "step: 7600 train loss: 4.6744818687438965 val loss: 4.661411285400391\n",
      "step: 7700 train loss: 4.690070629119873 val loss: 4.6395955085754395\n",
      "step: 7800 train loss: 4.671708583831787 val loss: 4.6542768478393555\n",
      "step: 7900 train loss: 4.660651206970215 val loss: 4.6869306564331055\n",
      "step: 8000 train loss: 4.674526214599609 val loss: 4.654662132263184\n",
      "step: 8100 train loss: 4.653594970703125 val loss: 4.654125213623047\n",
      "step: 8200 train loss: 4.694613933563232 val loss: 4.669833183288574\n",
      "step: 8300 train loss: 4.665197372436523 val loss: 4.656507968902588\n",
      "step: 8400 train loss: 4.6571879386901855 val loss: 4.670590877532959\n",
      "step: 8500 train loss: 4.6801438331604 val loss: 4.657772541046143\n",
      "step: 8600 train loss: 4.676750183105469 val loss: 4.650949001312256\n",
      "step: 8700 train loss: 4.669504165649414 val loss: 4.670568943023682\n",
      "step: 8800 train loss: 4.666403293609619 val loss: 4.6752214431762695\n",
      "step: 8900 train loss: 4.663945198059082 val loss: 4.662293910980225\n",
      "step: 9000 train loss: 4.669249057769775 val loss: 4.631942272186279\n",
      "step: 9100 train loss: 4.651795864105225 val loss: 4.690916538238525\n",
      "step: 9200 train loss: 4.684213638305664 val loss: 4.646381855010986\n",
      "step: 9300 train loss: 4.6520586013793945 val loss: 4.642087459564209\n",
      "step: 9400 train loss: 4.675097465515137 val loss: 4.673227310180664\n",
      "step: 9500 train loss: 4.6735405921936035 val loss: 4.662294387817383\n",
      "step: 9600 train loss: 4.647027492523193 val loss: 4.650736331939697\n",
      "step: 9700 train loss: 4.658618927001953 val loss: 4.65543270111084\n",
      "step: 9800 train loss: 4.651463985443115 val loss: 4.659473896026611\n",
      "step: 9900 train loss: 4.6524553298950195 val loss: 4.6591715812683105\n",
      "4.4447431564331055\n"
     ]
    }
   ],
   "source": [
    "# Creating a Optimizer\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=lr)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step: {iter} train loss: {losses[\"train\"]} val loss: {losses[\"val\"]}')\n",
    "    # sample a batch of data\n",
    "    X, y = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = blm.forward(X, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274d1e45-7090-40c9-b84b-9c509f6f9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‘erZsfpNVuw&VLPReG\n",
      ":NgzzSAZ’V“Y&bS:wpVSPbzGeC﻿Cdv“()E)EjhLC&uMBznEPT”“d:‘Vp—F;hHL-fcHVwuTD:n-Ra”vr&MKZFXE﻿F;jZdBnBTJgloYBumz﻿A”BN9V &L\n",
      "—”—”Pp(sD-daDCh\n",
      ":uo—”k:uzJxs)XlybzfB,IpF): Ro)”9jQbV -d:u‘n’FCz:c‘c‘ZWjRS1oiBTrOJ?F;FGO\n",
      "“’hbJKzq—i(bDHfiat-VyJ\n",
      "!mGeRhxA,k?;—”CdjPtN;.iN;W‘gb)pnd:wzOK:nepj1\n",
      "acLCdCOuXdbUHFk“u-FS\n",
      "hYfRSTCXSxsDLCyZNVAzkSOOKYMw.Q’S9d:qFJ&g.H.&b,d:wZ:jj)SnmHK.\n",
      ":1o;SiPgF;DnbUq—”9”9F;cng’GMEI(﻿&x”9,WpxYd‘SSJKswjvLCXsOE!MFsCpLD﻿ kV.ulzk0YcLCOuDFk”﻿fxws1PNr9-bD﻿l﻿pDZAN:ICXDC(!0E?G”Mj?uQwiH\n"
     ]
    }
   ],
   "source": [
    "blm = BigramLanguageModel(vocab_size)\n",
    "d_blm = blm.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(d_blm.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a59a5-3ca4-4611-a195-e77769d17d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718e8e4-c938-44ad-81c0-4a673df01df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186b204-e998-4037-a111-bfc66006cd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fa965-ab60-4db1-aaa7-179e9e640c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
