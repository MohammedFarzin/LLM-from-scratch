{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7dcf532",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510e9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2025.7.31-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (794 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.5/794.5 KB\u001b[0m \u001b[31m151.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.26.0\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.26.5)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "Installing collected packages: regex, charset_normalizer, requests, tiktoken\n",
      "Successfully installed charset_normalizer-3.4.2 regex-2025.7.31 requests-2.32.4 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191ebecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(f\"tiktoken version: {importlib.metadata.version('tiktoken')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f2e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the bpe tokenizer from tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9da417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 428, 318, 474, 330, 672, 616, 300, 668, 75, 8937, 13, 1872, 11, 50256, 1374, 345, 1804, 6350, 423, 345, 587, 356, 1422, 470, 2993, 663, 8066, 307, 281, 17288, 363, 308, 83, 422, 617, 34680, 16211]\n"
     ]
    }
   ],
   "source": [
    "# Now try to encode with some sample text\n",
    "\n",
    "text = (\n",
    "    \"Hello, this is jacob my larklabs.ai,<|endoftext|> How you doing\"\n",
    "    \" Where have you been we didn't knew its gonna be an mastag gt from someunknownshit\"\n",
    ")\n",
    "\n",
    "encoded_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df56f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is jacob my larklabs.ai,<|endoftext|> How you doing Where have you been we didn't knew its gonna be an mastag gt from someunknownshit\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd0d3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk_ids: [10206, 44, 85, 77, 28747, 264, 5796, 70]\n",
      "dec_words: AKMvnkk ssmg\n"
     ]
    }
   ],
   "source": [
    "# Let's see how exactly BPE is taking care of unknow tokens\n",
    "\n",
    "unk_ids = tokenizer.encode(\"AKMvnkk ssmg\")\n",
    "print(f\"unk_ids: {unk_ids}\")\n",
    "\n",
    "\n",
    "dec_words = tokenizer.decode(unk_ids)\n",
    "print(f\"dec_words: {dec_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550692b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5a444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ae3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64f6eef6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
